{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     self-signed certificate in certificate chain\n",
      "[nltk_data]     (_ssl.c:1000)>\n",
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     self-signed certificate in certificate chain\n",
      "[nltk_data]     (_ssl.c:1000)>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     self-signed certificate in certificate chain\n",
      "[nltk_data]     (_ssl.c:1000)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import zipfile\n",
    "import os\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from numpy.linalg import pinv\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/home/student/vishaka/collection_data.csv'\n",
    "chunk_size = 10000\n",
    "processed_chunks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "    chunk = chunk[['document_id', 'document_text']]\n",
    "    processed_chunks.append(chunk)\n",
    "collection_df = pd.concat(processed_chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>document_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The Manhattan Project and its atomic bomb help...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Essay on The Manhattan Project - The Manhattan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>The Manhattan Project was the name for a proje...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>versions of each volume as well as complementa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>The Manhattan Project. This once classified ph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8841817</th>\n",
       "      <td>8841818</td>\n",
       "      <td>When metal salts emit short wavelengths of vis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8841818</th>\n",
       "      <td>8841819</td>\n",
       "      <td>Thousands of people across the United States w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8841819</th>\n",
       "      <td>8841820</td>\n",
       "      <td>The recipe that creates blue, for example, inc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8841820</th>\n",
       "      <td>8841821</td>\n",
       "      <td>On Independence Days of yore, old-timey crowds...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8841821</th>\n",
       "      <td>8841822</td>\n",
       "      <td>View full size image. Behind the scenes of the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8841822 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         document_id                                      document_text\n",
       "0                  1  The Manhattan Project and its atomic bomb help...\n",
       "1                  2  Essay on The Manhattan Project - The Manhattan...\n",
       "2                  3  The Manhattan Project was the name for a proje...\n",
       "3                  4  versions of each volume as well as complementa...\n",
       "4                  5  The Manhattan Project. This once classified ph...\n",
       "...              ...                                                ...\n",
       "8841817      8841818  When metal salts emit short wavelengths of vis...\n",
       "8841818      8841819  Thousands of people across the United States w...\n",
       "8841819      8841820  The recipe that creates blue, for example, inc...\n",
       "8841820      8841821  On Independence Days of yore, old-timey crowds...\n",
       "8841821      8841822  View full size image. Behind the scenes of the...\n",
       "\n",
       "[8841822 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8841822, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"One of the main reasons Hanford was selected as a site for the Manhattan Project's B Reactor was its proximity to the Columbia River, the largest river flowing into the Pacific Ocean from the North American coast.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection_df['document_text'][8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessing tools\n",
    "ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "\n",
    "# Load SpaCy's English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Define preprocessing functions\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text by removing special characters, numbers, and extra spaces.\"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove digits\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"Remove stopwords from the text.\"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "def lemmatize_text_spacy(text):\n",
    "    \"\"\"Lemmatize text using SpaCy.\"\"\"\n",
    "    doc = nlp(text)  # Process text with SpaCy\n",
    "    lemmatized_words = [token.lemma_ for token in doc if not token.is_punct]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "def preprocess_text_spacy(text):\n",
    "    \"\"\"Apply all preprocessing steps to the text using SpaCy for lemmatization.\"\"\"\n",
    "    text = clean_text(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = lemmatize_text_spacy(text)\n",
    "    return text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_data_df=collection_df[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2464977/3035804518.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  collection_data_df['processed_document'] = collection_data_df['document_text'].apply(preprocess_text_spacy)\n"
     ]
    }
   ],
   "source": [
    "# Apply preprocessing to the 'document' column of the DataFrame\n",
    "collection_data_df['processed_document'] = collection_data_df['document_text'].apply(preprocess_text_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       document_text  \\\n",
      "0  The Manhattan Project and its atomic bomb help...   \n",
      "1  Essay on The Manhattan Project - The Manhattan...   \n",
      "2  The Manhattan Project was the name for a proje...   \n",
      "3  versions of each volume as well as complementa...   \n",
      "4  The Manhattan Project. This once classified ph...   \n",
      "\n",
      "                                  processed_document  \n",
      "0  manhattan project atomic bomb helped bring end...  \n",
      "1  essay manhattan project manhattan project manh...  \n",
      "2  manhattan project name project conduct world w...  \n",
      "3  version volume well complementary website firs...  \n",
      "4  manhattan project classify photograph feature ...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Display the first few rows after preprocessing\n",
    "print(collection_data_df[['document_text', 'processed_document']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the TSV file\n",
    "qrels_train_df = pd.read_csv('/home/student/vishaka/data/qrels.train.tsv', sep='\\t')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        1185869  0      0.1  1\n",
      "0       1185868  0       16  1\n",
      "1        597651  0       49  1\n",
      "2        403613  0       60  1\n",
      "3       1183785  0      389  1\n",
      "4        312651  0      616  1\n",
      "...         ... ..      ... ..\n",
      "532755    19285  0  8841362  1\n",
      "532756   558837  0  4989159  1\n",
      "532757   559149  0  8841547  1\n",
      "532758   706678  0  8841643  1\n",
      "532759   405466  0  8841735  1\n",
      "\n",
      "[532760 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "qrels_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "qrels_train_df.rename(columns={\n",
    "    qrels_train_df.columns[0]: 'query_id',\n",
    "    qrels_train_df.columns[1]: 'iteration',\n",
    "    qrels_train_df.columns[2]: 'document_id',\n",
    "    qrels_train_df.columns[3]: 'relevance_label'\n",
    "}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    document_id                                      document_text  \\\n",
      "0             1  The Manhattan Project and its atomic bomb help...   \n",
      "1             2  Essay on The Manhattan Project - The Manhattan...   \n",
      "2             3  The Manhattan Project was the name for a proje...   \n",
      "3             4  versions of each volume as well as complementa...   \n",
      "4             5  The Manhattan Project. This once classified ph...   \n",
      "5             6  Nor will it attempt to substitute for the extr...   \n",
      "6             7  Manhattan Project. The Manhattan Project was a...   \n",
      "7             8  In June 1942, the United States Army Corps of ...   \n",
      "8             9  One of the main reasons Hanford was selected a...   \n",
      "9            10  group discussions, community boards or panels ...   \n",
      "10           11  punishment designed to repair the damage done ...   \n",
      "11           12  Tutorial: Introduction to Restorative Justice....   \n",
      "12           13  Organize volunteer community panels, boards, o...   \n",
      "13           14  The purpose of this paper is to point out a nu...   \n",
      "14           15  Each of these types of communitiesÃ¢Â€Â”the geogr...   \n",
      "15           16  The approach is based on a theory of justice t...   \n",
      "16           17  Inherent in many peopleÃ¢Â€Â™s understanding of t...   \n",
      "17           18  Criminal justice, however, is not usually conc...   \n",
      "18           19  The circle includes a wide range of participan...   \n",
      "19           20  Phloem is a conductive (or vascular) tissue fo...   \n",
      "\n",
      "                                   processed_document  \n",
      "0   manhattan project atomic bomb helped bring end...  \n",
      "1   essay manhattan project manhattan project manh...  \n",
      "2   manhattan project name project conduct world w...  \n",
      "3   version volume well complementary website firs...  \n",
      "4   manhattan project classify photograph feature ...  \n",
      "5   attempt substitute extraordinarily rich litera...  \n",
      "6   manhattan project manhattan project research d...  \n",
      "7   june united states army corps engineersbegan m...  \n",
      "8   one main reason hanford select site manhattan ...  \n",
      "9   group discussion community board panel third p...  \n",
      "10  punishment design repair damage do victim comm...  \n",
      "11  tutorial introduction restorative justice rest...  \n",
      "12  organize volunteer community panel board commi...  \n",
      "13  purpose paper point number unresolved issue cr...  \n",
      "14  type communitiesÃ¢the geographic community vict...  \n",
      "15  approach base theory justice consider crime wr...  \n",
      "16  inherent many peopleÃ¢s understanding notion ad...  \n",
      "17  criminal justice however usually conceptualise...  \n",
      "18  circle include wide range participant include ...  \n",
      "19  phloem conductive vascular tissue find plant p...  \n"
     ]
    }
   ],
   "source": [
    "print(collection_data_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   query_id  iteration  document_id  relevance_label\n",
      "0   1185868          0           16                1\n",
      "1    597651          0           49                1\n",
      "2    403613          0           60                1\n",
      "3   1183785          0          389                1\n",
      "4    312651          0          616                1\n",
      "Unique Query IDs: [1185868  597651  403613 1183785  312651   80385  645590  645337  186154\n",
      "  457407  441383  683408 1164799  484187  460668  666321  182487  564233\n",
      "  455279  208108  733739 1164798  402608  443797  662502 1184679   14562\n",
      "  602162  545059  708236  310130  693161  186617  573027 1173772  541973\n",
      "  273090  441269  642237  503515  637443 1164796  749988  135841  295446\n",
      "  653051  691147  410621 1164795  598443  596451  651441  452286  308543\n",
      "  202126  114820  501778  531029  651110  594127 1164794  396032  705580\n",
      "  658203  387734  655102  224712  411732 1164793  605902  581014  559240\n",
      "  608711  535936  130335  147535 1164792  595576  569308  753706  627871\n",
      "  673608  510071  113839 1164791  460953  685235  650643 1183784 1164790\n",
      "   96740   26666  490046  485823  635632  534505  498612   85453  493122\n",
      "  512836  605764  748182  725274  401461  499565  641186  468434   31086\n",
      "  480268  742160  305154 1164787  688661 1173771  535243  477240  321187\n",
      "  755409  553140   88585  416278  652801  448861  609117  508975  686474\n",
      " 1164784  390443  342332  396120   82631  430171  422799  564701  677722\n",
      "  385458   50499  414754  113750  571562  235161   25587  544606  682818\n",
      "  634153  399213  519168  718889 1173450  543163  605393  269220  692976\n",
      "  486506  608727  695572  277532  312143  248994  414351  441108  624466\n",
      "  714655  496488  737121  422328  687420 1173082  593221  263642  142804\n",
      "  668386   78825  104990  655448  441765 1156360  736399  605763   95075\n",
      "  277756  609792  738038  159491  592326  435796 1153506  229136  733872\n",
      "  640439   21488  689943  481327 1152312  254248  479537 1151489  514738\n",
      "  528723  544980  581980  614513  734795  568909  584801  474108  716320\n",
      " 1149066  241084 1182233  468725  739932  639081    7273  587837  465177\n",
      "  733871  485338  505939  590073  106760  559856  436686  213460  465567\n",
      "  493098  742141  281107  695029   48283  687731  587682  654557  447428\n",
      "  585440  586257  969740  948848  925654  198547  809131 1028693  960666\n",
      "  955259  308652 1079288 1017862  953565  888704  994513  918353  251660\n",
      " 1078949  926938  948828 1028406  940456  816511  820002 1032812  844117\n",
      "  932391  799878 1005631  229071 1022329  932612  875691  962785  838291\n",
      "  579134 1068701  879099 1137044 1041678  837903 1017864  850072  818082\n",
      " 1072113  244811 1022408 1003774  898227  973685  885140  960871  893755\n",
      "  190073  843315  899688 1027789  787960  970505  947453  937923  796576\n",
      "  874432 1076548  808069  142134  814273  313342  880071  278436  328123\n",
      "  764473  819250  852302  103564  128157  867375  954211 1028746   18874\n",
      " 1062816  881523  828712  829755  827277  100098  285251 1073080  893177\n",
      "  924834 1049993 1017893  469979  860571  763522  584690  306992  974113\n",
      "  848583  935041  862916  982399  179290  966347  994926  802528  759692\n",
      "  834228 1041068 1005222  742633  772957 1056796  805484  820419  956493\n",
      " 1068319  958589  243512  898717 1170036  257120 1029043  835838  805378\n",
      "  945024  856897  882530  856432  937824  959163  706970  944691  804171\n",
      "  221571 1021231   86510  989324  892800  864466 1000127  312621  943496\n",
      " 1012121  778571  998448  187504  864272  805869  823834  910243  899631\n",
      " 1052746  124606  873759  789119  945951  947970  963268  846667 1031272\n",
      "  893638 1009934  915210  944600 1016443 1017532  925635  961913 1063532\n",
      "  720830 1079754  936729 1017877  999992  842743  846833  113996  894210\n",
      "  744398  469780  255080  274059 1074294  533861 1028153 1024915 1039740\n",
      "  891265  811584  608175  826317  980269  916707  798066  965277  846451\n",
      "  765794  517505  927707  933105  798309  270777 1008393  880463  916690\n",
      "  614512  272895  696762  941617  807546  582297  925325  865369    6072\n",
      "  828061  926634  814095 1011883  950428  929756  300739  900736  943668\n",
      "  243072  320418   31900  238138  186873  299783  350015  122752   17472\n",
      "  562406   92105   46453   12070  352391   33646  256938  196731  240153\n",
      "   45721   84453 1168119  242970  295973  285117  285771  334039  538037\n",
      "  554813  481191  868381  814884  852919  878725  998203  181054  293239\n",
      "  217395  105897  205515   36948  325893  134231   36553  591833  416310\n",
      "  437006  394535  534907  637313  392201  579208  244948  942219  733927\n",
      "  248414  836827 1010405 1055691 1022191  889504  879810  649720  837619\n",
      "  420629   34399  638564  397704 1022301  957866   10957  267129  315838\n",
      "   29087   31368  101505  132974   61050  142132  257047  240132  196219\n",
      "   79140  244170  293927  545724  704038  132780  139022  327083  299915\n",
      "  102622  192367 1177180   87960  251661  366484   34908  401029  582917\n",
      "  322686   91175  500517  303913  273236  402210  460653  714472  645350\n",
      "  466840  743269  713526  675218  254831  538220  710736 1174916  608730\n",
      "  754215  294842  431876  512653]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Extract the document IDs from the first 20,000 rows of collection_data\n",
    "doc_ids_within_20000 = collection_data_df[:20000]['document_id'].tolist()\n",
    "\n",
    "# Step 2: Filter queries_train_df based on document IDs\n",
    "filtered_queries = qrels_train_df[qrels_train_df['document_id'].isin(doc_ids_within_20000)]\n",
    "\n",
    "# Display the result\n",
    "print(filtered_queries.head())\n",
    "\n",
    "# If you want to fetch only the query_id column, you can do this:\n",
    "query_ids_within_20000 = filtered_queries['query_id'].unique()\n",
    "print(\"Unique Query IDs:\", query_ids_within_20000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_train_df = pd.read_csv('/home/student/vishaka/data/queries.train.tsv', sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     121352                                     define extreme\n",
      "0    634306           what does chattel mean on credit history\n",
      "1    920825            what was the great leap forward brainly\n",
      "2    510633                tattoo fixers how much does it cost\n",
      "3    737889                  what is decentralization process.\n",
      "4    278900  how many cars enter the la jolla concours d' e...\n",
      "5    674172                      what is a bank transit number\n",
      "6    303205     how much can i contribute to nondeductible ira\n",
      "7    570009         what are the four major groups of elements\n",
      "8    492875                              sanitizer temperature\n",
      "9     54528               blood clots in urine after menopause\n",
      "10   203218                                   highmark address\n",
      "11   473204     per sf cost in california for tenant build out\n",
      "12   738368                             what is delta in2ition\n",
      "13   507001              symptoms of an enlarged heart in dogs\n",
      "14   466926       number of times congress voted to repeal aca\n",
      "15   837327    what is the official color of army rank stripes\n",
      "16   406897      is colostrum beneficial for 3 week old calves\n",
      "17  1181095                how is a behavioral crisis defined?\n",
      "18   224811                        how does a firefly light up\n",
      "19   730323                       what is choice warranty plus\n"
     ]
    }
   ],
   "source": [
    "print(queries_train_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_train_df.rename(columns={\n",
    "    queries_train_df.columns[0]: 'query_id',\n",
    "    queries_train_df.columns[1]: 'query'\n",
    "}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        query_id                                              query\n",
      "0         634306           what does chattel mean on credit history\n",
      "1         920825            what was the great leap forward brainly\n",
      "2         510633                tattoo fixers how much does it cost\n",
      "3         737889                  what is decentralization process.\n",
      "4         278900  how many cars enter the la jolla concours d' e...\n",
      "...          ...                                                ...\n",
      "808725    633855             what does canada post regulations mean\n",
      "808726   1059728                            wholesale lularoe price\n",
      "808727    210839                      how can i watch the day after\n",
      "808728    908165              what to use instead of pgp in windows\n",
      "808729     50393     benefits of boiling lemons and drinking juice.\n",
      "\n",
      "[808730 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(queries_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        query_id                                              query\n",
      "1513      582297                              what can protein make\n",
      "3497      898717                  what std does metronidazole treat\n",
      "4872      312651  how much does an average person make for tutoring\n",
      "6564        7273                     What is a normal average pulse\n",
      "6690     1003774  where was the flush toilet invented during the...\n",
      "...          ...                                                ...\n",
      "798094     17472            amsterdam average temperatures by month\n",
      "798117    749988                            what is garlic used for\n",
      "802702    685235         what is a good home remedy for hemorrhoids\n",
      "805146    640439  what does it mean if you start bleeding during...\n",
      "807232    837903                 what is the origin of father's day\n",
      "\n",
      "[580 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "filtered_queries_df = queries_train_df[queries_train_df['query_id'].isin(query_ids_within_20000)]\n",
    "\n",
    "# Display the result\n",
    "print(filtered_queries_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_data_df.to_csv('collection_data_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "qrels_train_df.to_csv('qrels_train_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_queries_df .to_csv('filtered_queries_df .csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_extra_rows = 120\n",
    "extra_noise_df = filtered_queries_df.sample(n=num_extra_rows, replace=True, random_state=42)\n",
    "noisy_queries_df = pd.concat([filtered_queries_df, extra_noise_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>582297</td>\n",
       "      <td>what can protein make</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>898717</td>\n",
       "      <td>what std does metronidazole treat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>312651</td>\n",
       "      <td>how much does an average person make for tutoring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7273</td>\n",
       "      <td>What is a normal average pulse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1003774</td>\n",
       "      <td>where was the flush toilet invented during the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>924834</td>\n",
       "      <td>what windows version is this pc using</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>940456</td>\n",
       "      <td>when did qvc open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>748182</td>\n",
       "      <td>what is fms medical term</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>1174916</td>\n",
       "      <td>is it george harrison's birthday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>651110</td>\n",
       "      <td>what does the name asia mean</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>700 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     query_id                                              query\n",
       "0      582297                              what can protein make\n",
       "1      898717                  what std does metronidazole treat\n",
       "2      312651  how much does an average person make for tutoring\n",
       "3        7273                     What is a normal average pulse\n",
       "4     1003774  where was the flush toilet invented during the...\n",
       "..        ...                                                ...\n",
       "695    924834              what windows version is this pc using\n",
       "696    940456                                  when did qvc open\n",
       "697    748182                           what is fms medical term\n",
       "698   1174916                   is it george harrison's birthday\n",
       "699    651110                       what does the name asia mean\n",
       "\n",
       "[700 rows x 2 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_queries_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cur_decomposition(X, rank):\n",
    "    col_norms = np.sum(X**2, axis=0)\n",
    "    row_norms = np.sum(X**2, axis=1)\n",
    "    prob_cols = col_norms / np.sum(col_norms)\n",
    "    prob_rows = row_norms / np.sum(row_norms)\n",
    "    \n",
    "    selected_cols = np.random.choice(X.shape[1], rank, replace=False, p=prob_cols)\n",
    "    selected_rows = np.random.choice(X.shape[0], rank, replace=False, p=prob_rows)\n",
    "    \n",
    "    C = X[:, selected_cols]\n",
    "    R = X[selected_rows, :]\n",
    "    W = X[np.ix_(selected_rows, selected_cols)]\n",
    "    U = np.linalg.pinv(W)\n",
    "    return C, U, R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_gibbs_sampling(docs, num_topics, num_iter=1000, alpha=0.1, beta=0.1):\n",
    "    vocab = list(set(word for doc in docs for word in doc.split()))\n",
    "    term_doc_matrix = np.zeros((len(docs), len(vocab)))\n",
    "    \n",
    "    for i, doc in enumerate(docs):\n",
    "        for word in doc.split():\n",
    "            term_doc_matrix[i, vocab.index(word)] += 1\n",
    "    \n",
    "    num_docs, vocab_size = term_doc_matrix.shape\n",
    "    topic_assignments = np.random.randint(0, num_topics, size=(num_docs, vocab_size))\n",
    "    \n",
    "    doc_topic_counts = np.zeros((num_docs, num_topics))\n",
    "    topic_word_counts = np.zeros((num_topics, vocab_size))\n",
    "    topic_counts = np.zeros(num_topics)\n",
    "    \n",
    "    for d in range(num_docs):\n",
    "        for w in range(vocab_size):\n",
    "            topic = topic_assignments[d, w]\n",
    "            doc_topic_counts[d, topic] += term_doc_matrix[d, w]\n",
    "            topic_word_counts[topic, w] += term_doc_matrix[d, w]\n",
    "            topic_counts[topic] += term_doc_matrix[d, w]\n",
    "    \n",
    "    for _ in range(num_iter):\n",
    "        for d in range(num_docs):\n",
    "            for w in range(vocab_size):\n",
    "                word_count = term_doc_matrix[d, w]\n",
    "                if word_count == 0:\n",
    "                    continue\n",
    "                \n",
    "                topic = topic_assignments[d, w]\n",
    "                doc_topic_counts[d, topic] -= word_count\n",
    "                topic_word_counts[topic, w] -= word_count\n",
    "                topic_counts[topic] -= word_count\n",
    "                \n",
    "                topic_probs = ((doc_topic_counts[d, :] + alpha) * (topic_word_counts[:, w] + beta) / (topic_counts + beta * vocab_size))\n",
    "                topic_probs /= topic_probs.sum()\n",
    "                \n",
    "                new_topic = np.random.choice(num_topics, p=topic_probs)\n",
    "                topic_assignments[d, w] = new_topic\n",
    "                doc_topic_counts[d, new_topic] += word_count\n",
    "                topic_word_counts[new_topic, w] += word_count\n",
    "                topic_counts[new_topic] += word_count\n",
    "    \n",
    "    doc_topic_dist = (doc_topic_counts + alpha) / (doc_topic_counts.sum(axis=1, keepdims=True) + num_topics * alpha)\n",
    "    topic_word_dist = (topic_word_counts + beta) / (topic_word_counts.sum(axis=1, keepdims=True) + vocab_size * beta)\n",
    "    return doc_topic_dist, topic_word_dist, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_query_expansion(query, documents, model, top_n=3, original_weight=0.7, expanded_weight=0.3):\n",
    "    unique_terms = set(' '.join(documents).split())\n",
    "    term_embeddings = {term: model.encode(term) for term in unique_terms}\n",
    "    \n",
    "    query_terms = query.split()\n",
    "    query_embeddings = [model.encode(term) for term in query_terms]\n",
    "    query_embedding = np.mean(query_embeddings, axis=0)\n",
    "    \n",
    "    similarities = {term: cosine_similarity([query_embedding], [embedding])[0][0] for term, embedding in term_embeddings.items()}\n",
    "    expanded_terms = sorted(similarities, key=similarities.get, reverse=True)[:top_n]\n",
    "    \n",
    "    expanded_embeddings = [term_embeddings[term] for term in expanded_terms]\n",
    "    combined_embedding = original_weight * np.mean(query_embeddings, axis=0) + expanded_weight * np.mean(expanded_embeddings, axis=0)\n",
    "    \n",
    "    expanded_query_terms = set(query_terms).union(expanded_terms)\n",
    "    return combined_embedding, ' '.join(expanded_query_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents_with_cur(query, documents, model, lda_topics, lda_vocab, rank=10, top_n=5):\n",
    "    query_embedding, expanded_query = dynamic_query_expansion(query, documents, model)\n",
    "    print(f\"Expanded Query: '{expanded_query}'\\n\")\n",
    "    \n",
    "    document_embeddings = model.encode(documents)\n",
    "    C, U, R = cur_decomposition(document_embeddings, rank=rank)\n",
    "    reduced_document_embeddings = C @ U\n",
    "    reduced_query_embedding = query_embedding @ R.T\n",
    "    \n",
    "    similarities = cosine_similarity([reduced_query_embedding], reduced_document_embeddings).flatten()\n",
    "    top_indices = np.argsort(-similarities)[:top_n]\n",
    "    \n",
    "    print(\"LDA Topics Distribution for Top Documents:\")\n",
    "    for idx in top_indices:\n",
    "        doc_topics = lda_topics[idx]\n",
    "        print(f\"Document: {documents[idx]}\\nTopic Distribution: {doc_topics}\\n\")\n",
    "    \n",
    "    return top_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-vishaka",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
