{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cur_decomposition(X, rank):\n",
    "    \"\"\"Perform CUR decomposition on a matrix X with a specified rank.\"\"\"\n",
    "    col_norms = np.sum(X**2, axis=0)\n",
    "    row_norms = np.sum(X**2, axis=1)\n",
    "    prob_cols = col_norms / np.sum(col_norms)\n",
    "    prob_rows = row_norms / np.sum(row_norms)\n",
    "\n",
    "    selected_cols = np.random.choice(X.shape[1], rank, replace=False, p=prob_cols)\n",
    "    selected_rows = np.random.choice(X.shape[0], rank, replace=False, p=prob_rows)\n",
    "\n",
    "    C = X[:, selected_cols]\n",
    "    R = X[selected_rows, :]\n",
    "\n",
    "    W = X[np.ix_(selected_rows, selected_cols)]\n",
    "    U = np.linalg.pinv(W)\n",
    "\n",
    "    return C, U, R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_gibbs_sampling(docs, num_topics, num_iter=1000, alpha=0.1, beta=0.1):\n",
    "    vocab = list(set(word for doc in docs for word in doc.split()))\n",
    "    term_doc_matrix = np.zeros((len(docs), len(vocab)))\n",
    "\n",
    "    for i, doc in enumerate(docs):\n",
    "        for word in doc.split():\n",
    "            term_doc_matrix[i, vocab.index(word)] += 1\n",
    "\n",
    "    num_docs, vocab_size = term_doc_matrix.shape\n",
    "    topic_assignments = np.random.randint(0, num_topics, size=(num_docs, vocab_size))\n",
    "\n",
    "    doc_topic_counts = np.zeros((num_docs, num_topics))\n",
    "    topic_word_counts = np.zeros((num_topics, vocab_size))\n",
    "    topic_counts = np.zeros(num_topics)\n",
    "\n",
    "    for d in range(num_docs):\n",
    "        for w in range(vocab_size):\n",
    "            word_count = term_doc_matrix[d, w]\n",
    "            if word_count == 0:\n",
    "                continue\n",
    "\n",
    "            topic = topic_assignments[d, w]\n",
    "            doc_topic_counts[d, topic] += word_count\n",
    "            topic_word_counts[topic, w] += word_count\n",
    "            topic_counts[topic] += word_count\n",
    "\n",
    "    for _ in range(num_iter):\n",
    "        for d in range(num_docs):\n",
    "            for w in range(vocab_size):\n",
    "                word_count = term_doc_matrix[d, w]\n",
    "                if word_count == 0:\n",
    "                    continue\n",
    "\n",
    "                topic = topic_assignments[d, w]\n",
    "                doc_topic_counts[d, topic] -= word_count\n",
    "                topic_word_counts[topic, w] -= word_count\n",
    "                topic_counts[topic] -= word_count\n",
    "\n",
    "                topic_probs = (\n",
    "                    (doc_topic_counts[d, :] + alpha)\n",
    "                    * (topic_word_counts[:, w] + beta)\n",
    "                    / (topic_counts + beta * vocab_size)\n",
    "                )\n",
    "                topic_probs /= topic_probs.sum()\n",
    "\n",
    "                new_topic = np.random.choice(num_topics, p=topic_probs)\n",
    "                topic_assignments[d, w] = new_topic\n",
    "                doc_topic_counts[d, new_topic] += word_count\n",
    "                topic_word_counts[new_topic, w] += word_count\n",
    "                topic_counts[new_topic] += word_count\n",
    "\n",
    "    doc_topic_dist = (doc_topic_counts + alpha) / (doc_topic_counts.sum(axis=1, keepdims=True) + num_topics * alpha)\n",
    "    topic_word_dist = (topic_word_counts + beta) / (topic_word_counts.sum(axis=1, keepdims=True) + vocab_size * beta)\n",
    "\n",
    "    return doc_topic_dist, topic_word_dist, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_query_expansion(query, documents, model, top_n=3, original_weight=0.7, expanded_weight=0.3):\n",
    "    unique_terms = set(' '.join(documents).split())\n",
    "    term_embeddings = {term: model.encode(term) for term in unique_terms}\n",
    "\n",
    "    query_terms = query.split()\n",
    "    query_embeddings = [model.encode(term) for term in query_terms]\n",
    "\n",
    "    query_embedding = np.mean(query_embeddings, axis=0)\n",
    "    similarities = {\n",
    "        term: cosine_similarity([query_embedding], [embedding])[0][0]\n",
    "        for term, embedding in term_embeddings.items()\n",
    "    }\n",
    "    expanded_terms = sorted(similarities, key=similarities.get, reverse=True)[:top_n]\n",
    "\n",
    "    expanded_embeddings = [term_embeddings[term] for term in expanded_terms]\n",
    "    combined_embedding = (\n",
    "        original_weight * np.mean(query_embeddings, axis=0) +\n",
    "        expanded_weight * np.mean(expanded_embeddings, axis=0)\n",
    "    )\n",
    "\n",
    "    expanded_query_terms = set(query_terms).union(expanded_terms)\n",
    "    return combined_embedding, ' '.join(expanded_query_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_gibbs_sampling_with_cur(docs, num_topics, num_iter=1000, alpha=0.1, beta=0.1, rank=4, verbose=False):\n",
    "    # Step 1: Create Term-Document Matrix\n",
    "    vocab = list(set(word for doc in docs for word in doc.split()))\n",
    "    term_doc_matrix = np.zeros((len(docs), len(vocab)))\n",
    "\n",
    "    for i, doc in enumerate(docs):\n",
    "        for word in doc.split():\n",
    "            term_doc_matrix[i, vocab.index(word)] += 1\n",
    "\n",
    "    # Step 2: Apply CUR decomposition\n",
    "    C, U, R = cur_decomposition(term_doc_matrix, rank)\n",
    "    reduced_matrix = C @ U @ R\n",
    "\n",
    "    # Step 3: Use reduced_matrix for LDA\n",
    "    num_docs, vocab_size = reduced_matrix.shape\n",
    "    topic_assignments = np.random.randint(0, num_topics, size=(num_docs, vocab_size))\n",
    "\n",
    "    doc_topic_counts = np.zeros((num_docs, num_topics))\n",
    "    topic_word_counts = np.zeros((num_topics, vocab_size))\n",
    "    topic_counts = np.zeros(num_topics)\n",
    "\n",
    "    for d in range(num_docs):\n",
    "        for w in range(vocab_size):\n",
    "            topic = topic_assignments[d, w]\n",
    "            doc_topic_counts[d, topic] += reduced_matrix[d, w]\n",
    "            topic_word_counts[topic, w] += reduced_matrix[d, w]\n",
    "            topic_counts[topic] += reduced_matrix[d, w]\n",
    "\n",
    "    for iteration in range(num_iter):\n",
    "        if verbose:\n",
    "            print(f\"Iteration {iteration + 1}/{num_iter}\")\n",
    "\n",
    "        for d in range(num_docs):\n",
    "            if verbose and d % 10 == 0:  # Print progress for every 10 documents\n",
    "                print(f\"Processing document {d + 1}/{num_docs}\")\n",
    "\n",
    "            for w in range(vocab_size):\n",
    "                word_count = reduced_matrix[d, w]\n",
    "                if word_count == 0:\n",
    "                    continue\n",
    "\n",
    "                topic = topic_assignments[d, w]\n",
    "                doc_topic_counts[d, topic] -= word_count\n",
    "                topic_word_counts[topic, w] -= word_count\n",
    "                topic_counts[topic] -= word_count\n",
    "\n",
    "                topic_probs = (\n",
    "                    (doc_topic_counts[d, :] + alpha)\n",
    "                    * (topic_word_counts[:, w] + beta)\n",
    "                    / (topic_counts + beta * vocab_size)\n",
    "                )\n",
    "                topic_probs = np.maximum(topic_probs, 0)  # Ensure non-negative\n",
    "                prob_sum = topic_probs.sum()\n",
    "                if prob_sum > 0:\n",
    "                    topic_probs /= prob_sum\n",
    "                else:\n",
    "                    topic_probs = np.ones(num_topics) / num_topics\n",
    "\n",
    "                new_topic = np.random.choice(num_topics, p=topic_probs)\n",
    "                topic_assignments[d, w] = new_topic\n",
    "                doc_topic_counts[d, new_topic] += word_count\n",
    "                topic_word_counts[new_topic, w] += word_count\n",
    "                topic_counts[new_topic] += word_count\n",
    "\n",
    "    doc_topic_dist = (doc_topic_counts + alpha) / (doc_topic_counts.sum(axis=1, keepdims=True) + num_topics * alpha)\n",
    "    topic_word_dist = (topic_word_counts + beta) / (topic_word_counts.sum(axis=1, keepdims=True) + vocab_size * beta)\n",
    "\n",
    "    return doc_topic_dist, topic_word_dist, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents_with_cur(query, documents, model, lda_topics, lda_vocab, rank=10, top_n=5):\n",
    "    query_embedding, expanded_query = dynamic_query_expansion(query, documents, model)\n",
    "    print(f\"Expanded Query: '{expanded_query}'\\n\")\n",
    "    document_embeddings = model.encode(documents)\n",
    "\n",
    "    print(\"Applying CUR Decomposition to reduce document embeddings dimensionality...\")\n",
    "    C, U, R = cur_decomposition(document_embeddings, rank=rank)\n",
    "    reduced_document_embeddings = C @ U\n",
    "    reduced_query_embedding = query_embedding @ (R).T\n",
    "\n",
    "    similarities = cosine_similarity([reduced_query_embedding], reduced_document_embeddings).flatten()\n",
    "    top_indices = np.argsort(-similarities)[:top_n]\n",
    "\n",
    "    print(\"LDA Topics Distribution for Top Documents:\")\n",
    "    for idx in top_indices:\n",
    "        doc_topics = lda_topics[idx]\n",
    "        print(f\"Document: {documents[idx]}\\nTopic Distribution: {doc_topics}\\n\")\n",
    "\n",
    "    return top_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "collection_data_df=pd.read_csv('collection_data_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Set the proxy environment variables\n",
    "os.environ['HTTP_PROXY'] = 'socks5h://127.0.0.1:1080'\n",
    "os.environ['HTTPS_PROXY'] = 'socks5h://127.0.0.1:1080'\n",
    "\n",
    "# Explicitly download the model\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Wrap it with SentenceTransformer for compatibility\n",
    "sentence_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "print(\"Model loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the processed text for LDA and CUR\n",
    "documents = collection_data_df['processed_document'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load a SentenceTransformer model\n",
    "#model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "# Generate document embeddings\n",
    "document_embeddings = sentence_model.encode(documents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00369316  0.12447743 -0.03030166 ... -0.0348379  -0.04860682\n",
      "   0.0244041 ]\n",
      " [-0.04892228  0.07967986  0.00721633 ... -0.00510206 -0.04808084\n",
      "   0.00623703]\n",
      " [-0.0628251   0.03445498 -0.0747632  ...  0.01157742 -0.0669866\n",
      "   0.02865066]\n",
      " ...\n",
      " [ 0.06008795 -0.01425387 -0.02162606 ...  0.03436214  0.00333819\n",
      "  -0.07693569]\n",
      " [ 0.01510265 -0.07874505  0.06587734 ...  0.06211259  0.07921816\n",
      "  -0.09959285]\n",
      " [ 0.06404319  0.02151622 -0.02984285 ...  0.05427389  0.01936611\n",
      "  -0.07033923]]\n",
      "(20000, 384)\n"
     ]
    }
   ],
   "source": [
    "print(document_embeddings)\n",
    "print(document_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C matrix shape: (20000, 200)\n",
      "U matrix shape: (200, 200)\n",
      "R matrix shape: (200, 384)\n"
     ]
    }
   ],
   "source": [
    "# Apply CUR decomposition\n",
    "rank = 200  # You can tune this based on the dataset size\n",
    "C, U, R = cur_decomposition(document_embeddings, rank)\n",
    "\n",
    "print(\"C matrix shape:\", C.shape)\n",
    "print(\"U matrix shape:\", U.shape)\n",
    "print(\"R matrix shape:\", R.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m num_topics \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m  \u001b[38;5;66;03m# Number of topics to extract\u001b[39;00m\n\u001b[1;32m      3\u001b[0m num_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m  \u001b[38;5;66;03m# Number of iterations for Gibbs sampling\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m doc_topic_dist, topic_word_dist, vocab \u001b[38;5;241m=\u001b[39m \u001b[43mlda_gibbs_sampling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_topics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"print(\"Document-Topic Distribution Shape:\", doc_topic_dist.shape)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03mprint(\"Topic-Word Distribution Shape:\", topic_word_dist.shape)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03mprint(\"Vocabulary Size:\", len(vocab))\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 32\u001b[0m, in \u001b[0;36mlda_gibbs_sampling\u001b[0;34m(docs, num_topics, num_iter, alpha, beta)\u001b[0m\n\u001b[1;32m     30\u001b[0m word_count \u001b[38;5;241m=\u001b[39m term_doc_matrix[d, w]\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m word_count \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     34\u001b[0m topic \u001b[38;5;241m=\u001b[39m topic_assignments[d, w]\n\u001b[1;32m     35\u001b[0m doc_topic_counts[d, topic] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m word_count\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Perform LDA using the Gibbs sampling function\n",
    "num_topics = 5  # Number of topics to extract\n",
    "num_iter = 1000  # Number of iterations for Gibbs sampling\n",
    "\n",
    "doc_topic_dist, topic_word_dist, vocab = lda_gibbs_sampling(documents, num_topics, num_iter)\n",
    "\n",
    "\"\"\"print(\"Document-Topic Distribution Shape:\", doc_topic_dist.shape)\n",
    "print(\"Topic-Word Distribution Shape:\", topic_word_dist.shape)\n",
    "print(\"Vocabulary Size:\", len(vocab))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save CUR matrices\n",
    "np.save(\"C_matrix.npy\", C)\n",
    "np.save(\"U_matrix.npy\", U)\n",
    "np.save(\"R_matrix.npy\", R)\n",
    "\n",
    "# Save LDA distributions\n",
    "np.save(\"doc_topic_dist.npy\", doc_topic_dist)\n",
    "np.save(\"topic_word_dist.npy\", topic_word_dist)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-vishaka",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
