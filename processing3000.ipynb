{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     self-signed certificate in certificate chain\n",
      "[nltk_data]     (_ssl.c:1000)>\n",
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     self-signed certificate in certificate chain\n",
      "[nltk_data]     (_ssl.c:1000)>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     self-signed certificate in certificate chain\n",
      "[nltk_data]     (_ssl.c:1000)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import zipfile\n",
    "import os\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from numpy.linalg import pinv\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/home/student/vishaka/collection_data.csv'\n",
    "chunk_size = 10000\n",
    "processed_chunks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "    chunk = chunk[['document_id', 'document_text']]\n",
    "    processed_chunks.append(chunk)\n",
    "collection_df = pd.concat(processed_chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>document_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The Manhattan Project and its atomic bomb help...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Essay on The Manhattan Project - The Manhattan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>The Manhattan Project was the name for a proje...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>versions of each volume as well as complementa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>The Manhattan Project. This once classified ph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8841817</th>\n",
       "      <td>8841818</td>\n",
       "      <td>When metal salts emit short wavelengths of vis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8841818</th>\n",
       "      <td>8841819</td>\n",
       "      <td>Thousands of people across the United States w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8841819</th>\n",
       "      <td>8841820</td>\n",
       "      <td>The recipe that creates blue, for example, inc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8841820</th>\n",
       "      <td>8841821</td>\n",
       "      <td>On Independence Days of yore, old-timey crowds...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8841821</th>\n",
       "      <td>8841822</td>\n",
       "      <td>View full size image. Behind the scenes of the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8841822 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         document_id                                      document_text\n",
       "0                  1  The Manhattan Project and its atomic bomb help...\n",
       "1                  2  Essay on The Manhattan Project - The Manhattan...\n",
       "2                  3  The Manhattan Project was the name for a proje...\n",
       "3                  4  versions of each volume as well as complementa...\n",
       "4                  5  The Manhattan Project. This once classified ph...\n",
       "...              ...                                                ...\n",
       "8841817      8841818  When metal salts emit short wavelengths of vis...\n",
       "8841818      8841819  Thousands of people across the United States w...\n",
       "8841819      8841820  The recipe that creates blue, for example, inc...\n",
       "8841820      8841821  On Independence Days of yore, old-timey crowds...\n",
       "8841821      8841822  View full size image. Behind the scenes of the...\n",
       "\n",
       "[8841822 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8841822, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"One of the main reasons Hanford was selected as a site for the Manhattan Project's B Reactor was its proximity to the Columbia River, the largest river flowing into the Pacific Ocean from the North American coast.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection_df['document_text'][8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessing tools\n",
    "ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "\n",
    "# Load SpaCy's English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Define preprocessing functions\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text by removing special characters, numbers, and extra spaces.\"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove digits\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"Remove stopwords from the text.\"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "def lemmatize_text_spacy(text):\n",
    "    \"\"\"Lemmatize text using SpaCy.\"\"\"\n",
    "    doc = nlp(text)  # Process text with SpaCy\n",
    "    lemmatized_words = [token.lemma_ for token in doc if not token.is_punct]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "def preprocess_text_spacy(text):\n",
    "    \"\"\"Apply all preprocessing steps to the text using SpaCy for lemmatization.\"\"\"\n",
    "    text = clean_text(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = lemmatize_text_spacy(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_data_df=collection_df[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2686339/3035804518.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  collection_data_df['processed_document'] = collection_data_df['document_text'].apply(preprocess_text_spacy)\n"
     ]
    }
   ],
   "source": [
    "# Apply preprocessing to the 'document' column of the DataFrame\n",
    "collection_data_df['processed_document'] = collection_data_df['document_text'].apply(preprocess_text_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       document_text  \\\n",
      "0  The Manhattan Project and its atomic bomb help...   \n",
      "1  Essay on The Manhattan Project - The Manhattan...   \n",
      "2  The Manhattan Project was the name for a proje...   \n",
      "3  versions of each volume as well as complementa...   \n",
      "4  The Manhattan Project. This once classified ph...   \n",
      "\n",
      "                                  processed_document  \n",
      "0  manhattan project atomic bomb helped bring end...  \n",
      "1  essay manhattan project manhattan project manh...  \n",
      "2  manhattan project name project conduct world w...  \n",
      "3  version volume well complementary website firs...  \n",
      "4  manhattan project classify photograph feature ...  \n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows after preprocessing\n",
    "print(collection_data_df[['document_text', 'processed_document']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the TSV file\n",
    "qrels_train_df = pd.read_csv('/home/student/vishaka/data/qrels.train.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1185869</th>\n",
       "      <th>0</th>\n",
       "      <th>0.1</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1185868</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>597651</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>403613</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1183785</td>\n",
       "      <td>0</td>\n",
       "      <td>389</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>312651</td>\n",
       "      <td>0</td>\n",
       "      <td>616</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532755</th>\n",
       "      <td>19285</td>\n",
       "      <td>0</td>\n",
       "      <td>8841362</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532756</th>\n",
       "      <td>558837</td>\n",
       "      <td>0</td>\n",
       "      <td>4989159</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532757</th>\n",
       "      <td>559149</td>\n",
       "      <td>0</td>\n",
       "      <td>8841547</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532758</th>\n",
       "      <td>706678</td>\n",
       "      <td>0</td>\n",
       "      <td>8841643</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532759</th>\n",
       "      <td>405466</td>\n",
       "      <td>0</td>\n",
       "      <td>8841735</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>532760 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        1185869  0      0.1  1\n",
       "0       1185868  0       16  1\n",
       "1        597651  0       49  1\n",
       "2        403613  0       60  1\n",
       "3       1183785  0      389  1\n",
       "4        312651  0      616  1\n",
       "...         ... ..      ... ..\n",
       "532755    19285  0  8841362  1\n",
       "532756   558837  0  4989159  1\n",
       "532757   559149  0  8841547  1\n",
       "532758   706678  0  8841643  1\n",
       "532759   405466  0  8841735  1\n",
       "\n",
       "[532760 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qrels_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "qrels_train_df.rename(columns={\n",
    "    qrels_train_df.columns[0]: 'query_id',\n",
    "    qrels_train_df.columns[1]: 'iteration',\n",
    "    qrels_train_df.columns[2]: 'document_id',\n",
    "    qrels_train_df.columns[3]: 'relevance_label'\n",
    "}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    document_id                                      document_text  \\\n",
      "0             1  The Manhattan Project and its atomic bomb help...   \n",
      "1             2  Essay on The Manhattan Project - The Manhattan...   \n",
      "2             3  The Manhattan Project was the name for a proje...   \n",
      "3             4  versions of each volume as well as complementa...   \n",
      "4             5  The Manhattan Project. This once classified ph...   \n",
      "5             6  Nor will it attempt to substitute for the extr...   \n",
      "6             7  Manhattan Project. The Manhattan Project was a...   \n",
      "7             8  In June 1942, the United States Army Corps of ...   \n",
      "8             9  One of the main reasons Hanford was selected a...   \n",
      "9            10  group discussions, community boards or panels ...   \n",
      "10           11  punishment designed to repair the damage done ...   \n",
      "11           12  Tutorial: Introduction to Restorative Justice....   \n",
      "12           13  Organize volunteer community panels, boards, o...   \n",
      "13           14  The purpose of this paper is to point out a nu...   \n",
      "14           15  Each of these types of communitiesâthe geogr...   \n",
      "15           16  The approach is based on a theory of justice t...   \n",
      "16           17  Inherent in many peopleâs understanding of t...   \n",
      "17           18  Criminal justice, however, is not usually conc...   \n",
      "18           19  The circle includes a wide range of participan...   \n",
      "19           20  Phloem is a conductive (or vascular) tissue fo...   \n",
      "\n",
      "                                   processed_document  \n",
      "0   manhattan project atomic bomb helped bring end...  \n",
      "1   essay manhattan project manhattan project manh...  \n",
      "2   manhattan project name project conduct world w...  \n",
      "3   version volume well complementary website firs...  \n",
      "4   manhattan project classify photograph feature ...  \n",
      "5   attempt substitute extraordinarily rich litera...  \n",
      "6   manhattan project manhattan project research d...  \n",
      "7   june united states army corps engineersbegan m...  \n",
      "8   one main reason hanford select site manhattan ...  \n",
      "9   group discussion community board panel third p...  \n",
      "10  punishment design repair damage do victim comm...  \n",
      "11  tutorial introduction restorative justice rest...  \n",
      "12  organize volunteer community panel board commi...  \n",
      "13  purpose paper point number unresolved issue cr...  \n",
      "14  type communitiesâthe geographic community vict...  \n",
      "15  approach base theory justice consider crime wr...  \n",
      "16  inherent many peopleâs understanding notion ad...  \n",
      "17  criminal justice however usually conceptualise...  \n",
      "18  circle include wide range participant include ...  \n",
      "19  phloem conductive vascular tissue find plant p...  \n"
     ]
    }
   ],
   "source": [
    "print(collection_data_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   query_id  iteration  document_id  relevance_label\n",
      "0   1185868          0           16                1\n",
      "1    597651          0           49                1\n",
      "2    403613          0           60                1\n",
      "3   1183785          0          389                1\n",
      "4    312651          0          616                1\n",
      "Unique Query IDs: [1185868  597651  403613 1183785  312651   80385  645590  645337  186154\n",
      "  457407  441383  683408 1164799  484187  460668  666321  182487  564233\n",
      "  455279  208108  733739 1164798  543163  608727  695572  248994  441765\n",
      "  277756  738038  435796 1153506 1152312  479537  734795  654557  585440\n",
      "  955259 1028406 1032812  932391  932612  838291  850072 1022408  190073\n",
      "  787960  313342  819250  852302  881523  829755  827277  100098  584690\n",
      "  862916  966347  994926  759692 1170036 1029043 1021231  892800 1012121\n",
      "  187504  864272  945951  947970  944600  533861  608175  696762  925325\n",
      "  900736  320418  299783   17472   84453 1168119  852919  878725  205515\n",
      "  416310  637313  649720   10957  240132  704038  139022 1177180  401029\n",
      "  582917  500517  608730]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Extract the document IDs from the first 20,000 rows of collection_data\n",
    "doc_ids_within_3000 = collection_data_df[:3000]['document_id'].tolist()\n",
    "\n",
    "# Step 2: Filter queries_train_df based on document IDs\n",
    "filtered_queries = qrels_train_df[qrels_train_df['document_id'].isin(doc_ids_within_3000)]\n",
    "\n",
    "# Display the result\n",
    "print(filtered_queries.head())\n",
    "\n",
    "# If you want to fetch only the query_id column, you can do this:\n",
    "query_ids_within_3000 = filtered_queries['query_id'].unique()\n",
    "print(\"Unique Query IDs:\", query_ids_within_3000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_train_df = pd.read_csv('/home/student/vishaka/data/queries.train.tsv', sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     121352                                     define extreme\n",
      "0    634306           what does chattel mean on credit history\n",
      "1    920825            what was the great leap forward brainly\n",
      "2    510633                tattoo fixers how much does it cost\n",
      "3    737889                  what is decentralization process.\n",
      "4    278900  how many cars enter the la jolla concours d' e...\n",
      "5    674172                      what is a bank transit number\n",
      "6    303205     how much can i contribute to nondeductible ira\n",
      "7    570009         what are the four major groups of elements\n",
      "8    492875                              sanitizer temperature\n",
      "9     54528               blood clots in urine after menopause\n",
      "10   203218                                   highmark address\n",
      "11   473204     per sf cost in california for tenant build out\n",
      "12   738368                             what is delta in2ition\n",
      "13   507001              symptoms of an enlarged heart in dogs\n",
      "14   466926       number of times congress voted to repeal aca\n",
      "15   837327    what is the official color of army rank stripes\n",
      "16   406897      is colostrum beneficial for 3 week old calves\n",
      "17  1181095                how is a behavioral crisis defined?\n",
      "18   224811                        how does a firefly light up\n",
      "19   730323                       what is choice warranty plus\n"
     ]
    }
   ],
   "source": [
    "print(queries_train_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_train_df.rename(columns={\n",
    "    queries_train_df.columns[0]: 'query_id',\n",
    "    queries_train_df.columns[1]: 'query'\n",
    "}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        query_id                                              query\n",
      "0         634306           what does chattel mean on credit history\n",
      "1         920825            what was the great leap forward brainly\n",
      "2         510633                tattoo fixers how much does it cost\n",
      "3         737889                  what is decentralization process.\n",
      "4         278900  how many cars enter the la jolla concours d' e...\n",
      "...          ...                                                ...\n",
      "808725    633855             what does canada post regulations mean\n",
      "808726   1059728                            wholesale lularoe price\n",
      "808727    210839                      how can i watch the day after\n",
      "808728    908165              what to use instead of pgp in windows\n",
      "808729     50393     benefits of boiling lemons and drinking juice.\n",
      "\n",
      "[808730 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(queries_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        query_id                                              query\n",
      "4872      312651  how much does an average person make for tutoring\n",
      "10403     484187             put yourself on child support in texas\n",
      "24528     585440                  what causes arm and shoulder pain\n",
      "26592     892800                  what season does november fall in\n",
      "35272     455279                                mode of acquisition\n",
      "...          ...                                                ...\n",
      "776710   1164798   what causes elevated nitrate levels in aquariums\n",
      "781979    299783                 how many weeks does pregnancy take\n",
      "787967    925325  what would cause burning sensation in top of foot\n",
      "793664    190073                      foods to eat for healthy hair\n",
      "798094     17472            amsterdam average temperatures by month\n",
      "\n",
      "[93 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "filtered_queries_df = queries_train_df[queries_train_df['query_id'].isin(query_ids_within_3000)]\n",
    "\n",
    "# Display the result\n",
    "print(filtered_queries_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_data_df.to_csv('collection_data_df_3000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "qrels_train_df.to_csv('qrels_train_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_queries_df.to_csv('filtered_queries_df_3000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(93, 2)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_queries_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_extra_rows = 27\n",
    "extra_noise_df = filtered_queries_df.sample(n=num_extra_rows, replace=True, random_state=42)\n",
    "noisy_queries_df_3000 = pd.concat([filtered_queries_df, extra_noise_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>312651</td>\n",
       "      <td>how much does an average person make for tutoring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7273</td>\n",
       "      <td>What is a normal average pulse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>484187</td>\n",
       "      <td>put yourself on child support in texas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>585440</td>\n",
       "      <td>what causes arm and shoulder pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>892800</td>\n",
       "      <td>what season does november fall in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>14562</td>\n",
       "      <td>aggregate demand curve</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>1184679</td>\n",
       "      <td>an alpha helix is an example of which protein ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>248994</td>\n",
       "      <td>how long does a bruised rib hurt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>564233</td>\n",
       "      <td>what are rhetorical topics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>465177</td>\n",
       "      <td>normal adults resting heart rate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     query_id                                              query\n",
       "0      312651  how much does an average person make for tutoring\n",
       "1        7273                     What is a normal average pulse\n",
       "2      484187             put yourself on child support in texas\n",
       "3      585440                  what causes arm and shoulder pain\n",
       "4      892800                  what season does november fall in\n",
       "..        ...                                                ...\n",
       "175     14562                             aggregate demand curve\n",
       "176   1184679  an alpha helix is an example of which protein ...\n",
       "177    248994                   how long does a bruised rib hurt\n",
       "178    564233                         what are rhetorical topics\n",
       "179    465177                   normal adults resting heart rate\n",
       "\n",
       "[180 rows x 2 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_queries_df_3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_queries_df_3000.to_csv('noisy_queries_df_3000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cur_decomposition(X, rank):\n",
    "    col_norms = np.sum(X**2, axis=0)\n",
    "    row_norms = np.sum(X**2, axis=1)\n",
    "    prob_cols = col_norms / np.sum(col_norms)\n",
    "    prob_rows = row_norms / np.sum(row_norms)\n",
    "    \n",
    "    selected_cols = np.random.choice(X.shape[1], rank, replace=False, p=prob_cols)\n",
    "    selected_rows = np.random.choice(X.shape[0], rank, replace=False, p=prob_rows)\n",
    "    \n",
    "    C = X[:, selected_cols]\n",
    "    R = X[selected_rows, :]\n",
    "    W = X[np.ix_(selected_rows, selected_cols)]\n",
    "    U = np.linalg.pinv(W)\n",
    "    return C, U, R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_gibbs_sampling(docs, num_topics, num_iter=1000, alpha=0.1, beta=0.1):\n",
    "    vocab = list(set(word for doc in docs for word in doc.split()))\n",
    "    term_doc_matrix = np.zeros((len(docs), len(vocab)))\n",
    "    \n",
    "    for i, doc in enumerate(docs):\n",
    "        for word in doc.split():\n",
    "            term_doc_matrix[i, vocab.index(word)] += 1\n",
    "    \n",
    "    num_docs, vocab_size = term_doc_matrix.shape\n",
    "    topic_assignments = np.random.randint(0, num_topics, size=(num_docs, vocab_size))\n",
    "    \n",
    "    doc_topic_counts = np.zeros((num_docs, num_topics))\n",
    "    topic_word_counts = np.zeros((num_topics, vocab_size))\n",
    "    topic_counts = np.zeros(num_topics)\n",
    "    \n",
    "    for d in range(num_docs):\n",
    "        for w in range(vocab_size):\n",
    "            topic = topic_assignments[d, w]\n",
    "            doc_topic_counts[d, topic] += term_doc_matrix[d, w]\n",
    "            topic_word_counts[topic, w] += term_doc_matrix[d, w]\n",
    "            topic_counts[topic] += term_doc_matrix[d, w]\n",
    "    \n",
    "    for _ in range(num_iter):\n",
    "        for d in range(num_docs):\n",
    "            for w in range(vocab_size):\n",
    "                word_count = term_doc_matrix[d, w]\n",
    "                if word_count == 0:\n",
    "                    continue\n",
    "                \n",
    "                topic = topic_assignments[d, w]\n",
    "                doc_topic_counts[d, topic] -= word_count\n",
    "                topic_word_counts[topic, w] -= word_count\n",
    "                topic_counts[topic] -= word_count\n",
    "                \n",
    "                topic_probs = ((doc_topic_counts[d, :] + alpha) * (topic_word_counts[:, w] + beta) / (topic_counts + beta * vocab_size))\n",
    "                topic_probs /= topic_probs.sum()\n",
    "                \n",
    "                new_topic = np.random.choice(num_topics, p=topic_probs)\n",
    "                topic_assignments[d, w] = new_topic\n",
    "                doc_topic_counts[d, new_topic] += word_count\n",
    "                topic_word_counts[new_topic, w] += word_count\n",
    "                topic_counts[new_topic] += word_count\n",
    "    \n",
    "    doc_topic_dist = (doc_topic_counts + alpha) / (doc_topic_counts.sum(axis=1, keepdims=True) + num_topics * alpha)\n",
    "    topic_word_dist = (topic_word_counts + beta) / (topic_word_counts.sum(axis=1, keepdims=True) + vocab_size * beta)\n",
    "    return doc_topic_dist, topic_word_dist, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_query_expansion(query, documents, model, top_n=3, original_weight=0.7, expanded_weight=0.3):\n",
    "    unique_terms = set(' '.join(documents).split())\n",
    "    term_embeddings = {term: model.encode(term) for term in unique_terms}\n",
    "    \n",
    "    query_terms = query.split()\n",
    "    query_embeddings = [model.encode(term) for term in query_terms]\n",
    "    query_embedding = np.mean(query_embeddings, axis=0)\n",
    "    \n",
    "    similarities = {term: cosine_similarity([query_embedding], [embedding])[0][0] for term, embedding in term_embeddings.items()}\n",
    "    expanded_terms = sorted(similarities, key=similarities.get, reverse=True)[:top_n]\n",
    "    \n",
    "    expanded_embeddings = [term_embeddings[term] for term in expanded_terms]\n",
    "    combined_embedding = original_weight * np.mean(query_embeddings, axis=0) + expanded_weight * np.mean(expanded_embeddings, axis=0)\n",
    "    \n",
    "    expanded_query_terms = set(query_terms).union(expanded_terms)\n",
    "    return combined_embedding, ' '.join(expanded_query_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents_with_cur(query, documents, model, lda_topics, lda_vocab, rank=10, top_n=5):\n",
    "    query_embedding, expanded_query = dynamic_query_expansion(query, documents, model)\n",
    "    print(f\"Expanded Query: '{expanded_query}'\\n\")\n",
    "    \n",
    "    document_embeddings = model.encode(documents)\n",
    "    C, U, R = cur_decomposition(document_embeddings, rank=rank)\n",
    "    reduced_document_embeddings = C @ U\n",
    "    reduced_query_embedding = query_embedding @ R.T\n",
    "    \n",
    "    similarities = cosine_similarity([reduced_query_embedding], reduced_document_embeddings).flatten()\n",
    "    top_indices = np.argsort(-similarities)[:top_n]\n",
    "    \n",
    "    print(\"LDA Topics Distribution for Top Documents:\")\n",
    "    for idx in top_indices:\n",
    "        doc_topics = lda_topics[idx]\n",
    "        print(f\"Document: {documents[idx]}\\nTopic Distribution: {doc_topics}\\n\")\n",
    "    \n",
    "    return top_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-vishaka",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
